{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "qt_df= pd.read_csv(\"qt_metrics.csv\")\n",
    "qt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_df= pd.read_csv(\"openstack_metrics.csv\")\n",
    "op_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Merging dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_df['source'] = 'openstack'\n",
    "qt_df['source'] = 'qt'\n",
    "combined_df = pd.concat([op_df, qt_df], ignore_index=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# handeling commits\n",
    "Idea was to make a database, to determine average bug per commits, but as each commit is unique it seems that the commit:id is unqiue every time, then it means that we can nto use it to track one specifci persons commit patten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Total rows: {combined_df.shape[0]}\")\n",
    "\n",
    "\n",
    "print(f\"Unique commit_id count: {combined_df['author_date'].nunique()}\")\n",
    "\n",
    "\n",
    "print(f\"NaN in commit_id: {combined_df['author_date'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"revd\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# authour date and Handeling bug count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['author_date'] = pd.to_datetime(combined_df['author_date'], unit='s')\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['year'] = combined_df['author_date'].dt.year\n",
    "combined_df['month'] = combined_df['author_date'].dt.month\n",
    "combined_df['day_of_week'] = combined_df['author_date'].dt.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "combined_df['hour'] = combined_df['author_date'].dt.hour\n",
    "def categorize_time(hour):\n",
    "    if 6 <= hour < 12:\n",
    "        return 'morning'\n",
    "    elif 12 <= hour < 18:\n",
    "        return 'afternoon'\n",
    "    elif 18 <= hour < 24:\n",
    "        return 'evening'\n",
    "    else:\n",
    "        return 'night'\n",
    "\n",
    "combined_df['time_of_day'] = combined_df['hour'].apply(categorize_time)\n",
    "\n",
    "combined_df = combined_df.sort_values(by='author_date')  # Sort by time\n",
    "combined_df['time_since_last_commit'] = combined_df['author_date'].diff().dt.total_seconds()\n",
    "combined_df['time_since_last_commit_hours'] = combined_df['time_since_last_commit'] / 3600  # Convert to hours\n",
    "combined_df['time_since_last_commit_days'] = combined_df['time_since_last_commit'] / (3600 * 24)  # Convert to days\n",
    "combined_df=combined_df.drop(columns=['time_since_last_commit','time_since_last_commit_hours'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "combined_df['time_since_last_commit_days'].fillna(0, inplace=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['bugcount'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is no indcation that missing bug counts are indicative of 0 bugs, I am going to remove them. I choose to do this over imputation as there are a lot of Nan in relation to other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.dropna(subset=['bugcount'], inplace=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"bug\"]= combined_df[\"bugcount\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "columns = [\"bug\"] + [col for col in combined_df.columns if col != \"bug\"]\n",
    "combined_df = combined_df[columns]\n",
    "combined_df.drop(axis=0, columns= ['bugcount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'day_of_week' in combined_df.columns:\n",
    "    avg_bug_day = combined_df.groupby('day_of_week')['bug'].mean()\n",
    "    print(\"Average Bug by Day of Week:\")\n",
    "    print(avg_bug_day)\n",
    "\n",
    "if 'time_of_day' in combined_df.columns:\n",
    "    avg_bug_time = combined_df.groupby('time_of_day')['bug'].mean()\n",
    "    print(\"\\nAverage Bug by Time of Day:\")\n",
    "    print(avg_bug_time)\n",
    "\n",
    "if 'month' in combined_df.columns:\n",
    "    avg_bug_time = combined_df.groupby('month')['bug'].mean()\n",
    "    print(\"\\nAverage Bug by month:\")\n",
    "    print(avg_bug_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "combined_df['time_of_day_encoded'] = label_encoder.fit_transform(combined_df['time_of_day'])\n",
    "\n",
    "correlation_day = combined_df['day_of_week'].corr(combined_df['bugcount'])\n",
    "correlation_time_label = combined_df['time_of_day_encoded'].corr(combined_df['bugcount'])\n",
    "correlation_month = combined_df['month'].corr(combined_df['bugcount'])\n",
    "correlation_year = combined_df['year'].corr(combined_df['bugcount'])\n",
    "\n",
    "print(f\"Correlation with day_of_week: {correlation_day}\")\n",
    "print(f\"Correlation with time_of_day (label encoded): {correlation_time_label}\")\n",
    "print(f\"Correlation with day_of_week: {correlation_month}\")\n",
    "print(f\"Correlation with time_of_day (label encoded): {correlation_year}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not worth keepign the time varaibles they have low correlation and low predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"fixcount\"].value_counts()\n",
    "#combined_df[\"fixcount\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasonable assumption that if there are no fixes then nothing is inputed to fixcount. As such we will set NA values for fixcount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"fixcount\"]= combined_df[\"fixcount\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts = combined_df.isna().sum()\n",
    "na_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check how many rows lost if we drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_df=combined_df\n",
    "cl_df.dropna(inplace=True)\n",
    "cl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_df[\"revd\"].value_counts()\n",
    "cl_df=cl_df.drop(columns=[\"revd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "if 'time_of_day' in cl_df.columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    cl_df['time_of_day'] = label_encoder.fit_transform(cl_df['time_of_day'])\n",
    "    \n",
    "    label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "    print(\"Label encoding mapping:\", label_mapping)\n",
    "else:\n",
    "    print(\"Column 'time_of_day' does not exist in the DataFrame.\")\n",
    "\n",
    "\n",
    "\n",
    "if 'source' in cl_df.columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    cl_df['source'] = label_encoder.fit_transform(cl_df['source'])\n",
    "    \n",
    "    label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "    print(\"Label encoding mapping:\", label_mapping)\n",
    "else:\n",
    "    print(\"Column 'time_of_day' does not exist in the DataFrame.\")\n",
    "cl_df = cl_df.drop(columns=['time_of_day_encoded'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "cl_df=cl_df.drop(columns= [\"author_date\", \"commit_id\",])\n",
    "\n",
    "feature_columns = cl_df.drop(columns=['bug', 'bugcount','source']).columns\n",
    "X = cl_df[feature_columns]\n",
    "y = cl_df['bug']\n",
    "\n",
    "rf = RandomForestClassifier(random_state=8)\n",
    "rf.fit(X, y)\n",
    "\n",
    "feature_importance = pd.Series(rf.feature_importances_, index=feature_columns).sort_values(ascending=False)\n",
    "feature_importance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "important_features = feature_importance[feature_importance >= threshold].index\n",
    "X = X[important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correlation_matrix = cl_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import xgboost\n",
    "%pip install shap\n",
    "import shap\n",
    "\n",
    "\n",
    "X = cl_df.drop(columns=['bug', 'bugcount', 'source'])\n",
    "y = cl_df['bug'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\n",
    "model = xgboost.XGBClassifier().fit(X_train, y_train)\n",
    "# model = RandomForestClassifier(random_state=8).fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy}\")\n",
    "\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values, max_display=39)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display=39)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "           \n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "\n",
    "X_pca_transformed = pca.transform(X_train)\n",
    "\n",
    "df_pca_components = pd.DataFrame(X_pca_transformed, columns=[f'PC{i+1}' for i in range(X_pca_transformed.shape[1])])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--', color='b')\n",
    "plt.title('Elbow Diagram: Explained Variance vs. Number of Components')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "pca.fit(X_train)\n",
    "\n",
    "X_pca_transformed = pca.transform(X_train)\n",
    "\n",
    "X_train_post_pca = pd.DataFrame(X_pca_transformed, columns=[f'PC{i+1}' for i in range(X_pca_transformed.shape[1])])\n",
    "\n",
    "X_train_post_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pca=pca.transform(X_test)\n",
    "test_pca = pd.DataFrame(test_pca, columns=[f'PC{i+1}' for i in range(test_pca.shape[1])])\n",
    "test_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier().fit(X_train_post_pca,y_train)\n",
    "\n",
    "\n",
    "y_pred = rf.predict(test_pca)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not doing pca, as it does not improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing models with varying feature inclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, Perceptron\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "!pip install catboost\n",
    "from catboost import CatBoostClassifier\n",
    "!pip install lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "!pip install ngboost\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "!pip install imbalanced-learn\n",
    "\n",
    "!pip install --upgrade scikit-learn\n",
    "!pip install --upgrade imbalanced-learn\n",
    "\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier, BalancedBaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "!pip install rgf-python\n",
    "\n",
    "from rgf.sklearn import RGFClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#rom tensorflow.keras.layers import Dense\n",
    "\n",
    "!pip install pytorch-tabnet\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "poly_log_reg = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree=2)),\n",
    "    ('log_reg', LogisticRegression(max_iter=100000))\n",
    "])\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "#!pip install deep-forest\n",
    "#from deepforest import CascadeForestClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "models = [\n",
    "    #('TabNetClassifier', TabNetClassifier(verbose=0)),\n",
    "    ('XGBRFClassifier', XGBRFClassifier()),\n",
    "    ('XGBClassifier', XGBClassifier()),\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=100000)),\n",
    "    #('Cascade Forest', CascadeForestClassifier()),\n",
    "    ('Gaussian Process Classifier', GaussianProcessClassifier()),\n",
    "    ('Nearest Centroid', NearestCentroid()),\n",
    "    ('Ridge Classifier', RidgeClassifier()),\n",
    "    ('LightGBM DART', LGBMClassifier(boosting_type='dart')),\n",
    "    ('LightGBM GOSS', LGBMClassifier(boosting_type='goss')),\n",
    "    ('OneVsRest with SVC', OneVsRestClassifier(SVC())),\n",
    "    ('OneVsOne with SVC', OneVsOneClassifier(SVC())),\n",
    "    #('AutoSklearnClassifier', autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=600)),\n",
    "    #('Multinomial Naive Bayes', MultinomialNB()),\n",
    "    ('Polynomial Logistic Regression', poly_log_reg),\n",
    "    ('Passive-Aggressive Classifier', PassiveAggressiveClassifier(max_iter=1000)),\n",
    "    ('SGD Classifier', SGDClassifier(max_iter=100000)),\n",
    "    # ('SVM Linear', SVC(kernel='linear')),\n",
    "    # ('SVM Polynomial (degree 2)', SVC(kernel='poly', degree=2)),\n",
    "    # ('SVM Polynomial (degree 3)', SVC(kernel='poly', degree=3)),\n",
    "    # ('SVM Polynomial (degree 4)', SVC(kernel='poly', degree=4)),\n",
    "    # ('SVM RBF', SVC(kernel='rbf')),\n",
    "    # ('Linear SVC', LinearSVC(dual='auto', max_iter=100000)),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('Gaussian Naive Bayes', GaussianNB()),\n",
    "    ('Bernoulli Naive Bayes', BernoulliNB()),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Extra Tree Classifier', ExtraTreeClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier()),\n",
    "    ('AdaBoost', AdaBoostClassifier()),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier()),\n",
    "    ('Extra Trees Classifier', ExtraTreesClassifier()),\n",
    "    ('Bagging Classifier', BaggingClassifier()),\n",
    "    ('Linear Discriminant Analysis', LinearDiscriminantAnalysis()),\n",
    "    ('Quadratic Discriminant Analysis', QuadraticDiscriminantAnalysis()),\n",
    "    ('CatBoostClassifier', CatBoostClassifier(verbose=0)),\n",
    "    ('LGBMClassifier', LGBMClassifier()),\n",
    "    ('HistGradientBoostingClassifier', HistGradientBoostingClassifier()),\n",
    "    ('Perceptron', Perceptron(max_iter=100000)),\n",
    "    ('BalancedRandomForestClassifier', BalancedRandomForestClassifier()),\n",
    "    ('BalancedBaggingClassifier', BalancedBaggingClassifier()),\n",
    "    ('CalibratedClassifierCV', CalibratedClassifierCV(estimator=LogisticRegression())),\n",
    "    ('MLPClassifier', MLPClassifier(max_iter=100000)),\n",
    "    ('RUSBoostClassifier', RUSBoostClassifier()),\n",
    "    ('RGFClassifier', RGFClassifier()),\n",
    "    ('Voting Classifier', VotingClassifier(estimators=[\n",
    "        ('lr', LogisticRegression()), \n",
    "        ('rf', RandomForestClassifier())\n",
    "        # ('svc', SVC(kernel='linear'))\n",
    "    ])),\n",
    "    ('Stacking Classifier', StackingClassifier(estimators=[\n",
    "        ('rf', RandomForestClassifier()),\n",
    "        ('knn', KNeighborsClassifier())\n",
    "    ], final_estimator=LogisticRegression()))\n",
    "]\n",
    "\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=8)\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models:\n",
    "    if name == 'TabNetClassifier':\n",
    "        X_train_np = X_train.values  # Convert DataFrame to NumPy array\n",
    "        cv_scores = cross_val_score(model, X_train_np, y_train, cv=kf, scoring='accuracy')\n",
    "    else:\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "\n",
    "    mean_accuracy = np.mean(cv_scores)\n",
    "    print(f\"{name} accuracy: {mean_accuracy} \")\n",
    "    std_accuracy = np.std(cv_scores)\n",
    "\n",
    "    results.append((name, mean_accuracy, std_accuracy))\n",
    "\n",
    "sorted_results = sorted(results, key=lambda item: item[1], reverse=True)\n",
    "\n",
    "summary = []\n",
    "for rank, (model_name, mean_accuracy, std_accuracy) in enumerate(sorted_results):\n",
    "    summary.append([model_name, rank + 1, mean_accuracy, std_accuracy])\n",
    "\n",
    "summary_df = pd.DataFrame(summary, columns=['Model', 'Rank', 'Mean Accuracy', 'Std Deviation'])\n",
    "\n",
    "summary_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv', index=False)\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "all_columns = X.columns\n",
    "\n",
    "categorical_columns = ['time_of_day', 'day_of_week', 'month', 'hour', 'year', 'time_of_day_encoded']\n",
    "numerical_columns = [col for col in all_columns if col not in categorical_columns]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X[numerical_columns] = scaler.fit_transform(X[numerical_columns])\n",
    "\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "models = [\n",
    "    #('TabNetClassifier', TabNetClassifier(verbose=0)),\n",
    "    ('XGBRFClassifier', XGBRFClassifier()),\n",
    "    ('XGBClassifier', XGBClassifier()),\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=100000)),\n",
    "    #('Cascade Forest', CascadeForestClassifier()),\n",
    "    ('Gaussian Process Classifier', GaussianProcessClassifier()),\n",
    "    ('Nearest Centroid', NearestCentroid()),\n",
    "    ('Ridge Classifier', RidgeClassifier()),\n",
    "    ('LightGBM DART', LGBMClassifier(boosting_type='dart')),\n",
    "    ('LightGBM GOSS', LGBMClassifier(boosting_type='goss')),\n",
    "    ('OneVsRest with SVC', OneVsRestClassifier(SVC())),\n",
    "    ('OneVsOne with SVC', OneVsOneClassifier(SVC())),\n",
    "    #('AutoSklearnClassifier', autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=600)),\n",
    "    #('Multinomial Naive Bayes', MultinomialNB()),\n",
    "    ('Polynomial Logistic Regression', poly_log_reg),\n",
    "    ('Passive-Aggressive Classifier', PassiveAggressiveClassifier(max_iter=1000)),\n",
    "    ('SGD Classifier', SGDClassifier(max_iter=100000)),\n",
    "    ('SVM Linear', SVC(kernel='linear')),\n",
    "    ('SVM Polynomial (degree 2)', SVC(kernel='poly', degree=2)),\n",
    "    ('SVM Polynomial (degree 3)', SVC(kernel='poly', degree=3)),\n",
    "    ('SVM Polynomial (degree 4)', SVC(kernel='poly', degree=4)),\n",
    "    ('SVM RBF', SVC(kernel='rbf')),\n",
    "    ('Linear SVC', LinearSVC(dual='auto', max_iter=100000)),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('Gaussian Naive Bayes', GaussianNB()),\n",
    "    ('Bernoulli Naive Bayes', BernoulliNB()),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Extra Tree Classifier', ExtraTreeClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier()),\n",
    "    ('AdaBoost', AdaBoostClassifier()),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier()),\n",
    "    ('Extra Trees Classifier', ExtraTreesClassifier()),\n",
    "    ('Bagging Classifier', BaggingClassifier()),\n",
    "    ('Linear Discriminant Analysis', LinearDiscriminantAnalysis()),\n",
    "    ('Quadratic Discriminant Analysis', QuadraticDiscriminantAnalysis()),\n",
    "    ('CatBoostClassifier', CatBoostClassifier(verbose=0)),\n",
    "    ('LGBMClassifier', LGBMClassifier()),\n",
    "    ('HistGradientBoostingClassifier', HistGradientBoostingClassifier()),\n",
    "    ('Perceptron', Perceptron(max_iter=100000)),\n",
    "    ('BalancedRandomForestClassifier', BalancedRandomForestClassifier()),\n",
    "    ('BalancedBaggingClassifier', BalancedBaggingClassifier()),\n",
    "    ('CalibratedClassifierCV', CalibratedClassifierCV(estimator=LogisticRegression())),\n",
    "    # ('MLPClassifier', MLPClassifier(max_iter=100000)),\n",
    "    ('RUSBoostClassifier', RUSBoostClassifier()),\n",
    "    ('RGFClassifier', RGFClassifier()),\n",
    "    ('Voting Classifier', VotingClassifier(estimators=[\n",
    "        ('lr', LogisticRegression()), \n",
    "        ('rf', RandomForestClassifier())\n",
    "        # ('svc', SVC(kernel='linear'))\n",
    "    ])),\n",
    "    ('Stacking Classifier', StackingClassifier(estimators=[\n",
    "        ('rf', RandomForestClassifier()),\n",
    "        ('knn', KNeighborsClassifier())\n",
    "    ], final_estimator=LogisticRegression()))\n",
    "]\n",
    "\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=8)\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models:\n",
    "    if name == 'TabNetClassifier':\n",
    "        X_train_np = X_train.values  # Convert DataFrame to NumPy array\n",
    "        cv_scores = cross_val_score(model, X_train_np, y_train, cv=kf, scoring='accuracy')\n",
    "    else:\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "\n",
    "    mean_accuracy = np.mean(cv_scores)\n",
    "    print(f\"{name} accuracy: {mean_accuracy} \")\n",
    "    std_accuracy = np.std(cv_scores)\n",
    "\n",
    "    results.append((name, mean_accuracy, std_accuracy))\n",
    "\n",
    "sorted_results = sorted(results, key=lambda item: item[1], reverse=True)\n",
    "\n",
    "summary = []\n",
    "for rank, (model_name, mean_accuracy, std_accuracy) in enumerate(sorted_results):\n",
    "    summary.append([model_name, rank + 1, mean_accuracy, std_accuracy])\n",
    "\n",
    "summary_df = pd.DataFrame(summary, columns=['Model', 'Rank', 'Mean Accuracy', 'Std Deviation'])\n",
    "\n",
    "summary_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train_norm.csv', index=False)\n",
    "X_test.to_csv('X_test_norm.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pca + normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "combined_data = pd.concat([X_train_post_pca, test_pca], axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "normalized_data = scaler.fit_transform(combined_data)\n",
    "\n",
    "X_train_normalized = normalized_data[:len(X_train_post_pca)] \n",
    "X_test_normalized = normalized_data[len(X_train_post_pca):]  \n",
    "\n",
    "X_train_normalized_df = pd.DataFrame(X_train_normalized, columns=X_train_post_pca.columns)\n",
    "X_test_normalized_df = pd.DataFrame(X_test_normalized, columns=test_pca.columns)\n",
    "\n",
    "print(\"X_train_normalized shape:\", X_train_normalized_df.shape)\n",
    "print(\"X_test_normalized shape:\", X_test_normalized_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 8\n",
    "models = [\n",
    "    #('TabNetClassifier', TabNetClassifier(verbose=0)),\n",
    "    ('XGBRFClassifier', XGBRFClassifier(random_seed=seed)),\n",
    "    ('XGBClassifier', XGBClassifier(random_seed=seed)),\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=100000)),\n",
    "    #('Cascade Forest', CascadeForestClassifier(random_seed=seed)),\n",
    "    ('Gaussian Process Classifier', GaussianProcessClassifier()),\n",
    "    ('Nearest Centroid', NearestCentroid()),\n",
    "    ('Ridge Classifier', RidgeClassifier()),\n",
    "    ('LightGBM DART', LGBMClassifier(boosting_type='dart', random_seed=seed)),\n",
    "    ('LightGBM GOSS', LGBMClassifier(boosting_type='goss', random_seed=seed)),\n",
    "    ('OneVsRest with SVC', OneVsRestClassifier(SVC())),\n",
    "    ('OneVsOne with SVC', OneVsOneClassifier(SVC())),\n",
    "    #('AutoSklearnClassifier', autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=600)),\n",
    "    #('Multinomial Naive Bayes', MultinomialNB()),\n",
    "    ('Polynomial Logistic Regression', poly_log_reg),\n",
    "    ('Passive-Aggressive Classifier', PassiveAggressiveClassifier(max_iter=1000)),\n",
    "    ('SGD Classifier', SGDClassifier(max_iter=100000)),\n",
    "    ('SVM Linear', SVC(kernel='linear', )),\n",
    "    ('SVM Polynomial (degree 2)', SVC(kernel='poly', degree=2, random_state=seed)),\n",
    "    ('SVM Polynomial (degree 3)', SVC(kernel='poly', degree=3, random_state=seed)),\n",
    "    ('SVM Polynomial (degree 4)', SVC(kernel='poly', degree=4, random_state=seed)),\n",
    "    ('SVM RBF', SVC(kernel='rbf', random_state=seed)),\n",
    "    ('Linear SVC', LinearSVC(dual='auto', max_iter=100000, random_state=seed)),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('Gaussian Naive Bayes', GaussianNB()),\n",
    "    ('Bernoulli Naive Bayes', BernoulliNB()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=seed)),\n",
    "    ('Extra Tree Classifier', ExtraTreeClassifier(random_state=seed)),\n",
    "    ('Random Forest', RandomForestClassifier(random_state=seed)),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=seed)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=seed)),\n",
    "    ('Extra Trees Classifier', ExtraTreesClassifier(random_state=seed)),\n",
    "    ('Bagging Classifier', BaggingClassifier(random_state=seed)),\n",
    "    ('Linear Discriminant Analysis', LinearDiscriminantAnalysis()),\n",
    "    ('Quadratic Discriminant Analysis', QuadraticDiscriminantAnalysis()),\n",
    "    ('CatBoostClassifier', CatBoostClassifier(random_seed=seed, verbose=0)),\n",
    "    ('LGBMClassifier', LGBMClassifier(random_seed=seed)),\n",
    "    ('HistGradientBoostingClassifier', HistGradientBoostingClassifier(random_state=seed)),\n",
    "    ('Perceptron', Perceptron(max_iter=100000)),\n",
    "    ('BalancedRandomForestClassifier', BalancedRandomForestClassifier(random_state=seed)),\n",
    "    ('BalancedBaggingClassifier', BalancedBaggingClassifier(random_state=seed)),\n",
    "    ('CalibratedClassifierCV', CalibratedClassifierCV(estimator=LogisticRegression(random_state=seed))),\n",
    "    # ('MLPClassifier', MLPClassifier(max_iter=100000)),\n",
    "    ('RUSBoostClassifier', RUSBoostClassifier(random_state=seed)),\n",
    "    ('RGFClassifier', RGFClassifier()),\n",
    "    ('Voting Classifier', VotingClassifier(estimators=[\n",
    "        ('lr', LogisticRegression(random_state=seed)), \n",
    "        ('rf', RandomForestClassifier(random_state=seed))\n",
    "        # ('svc', SVC(kernel='linear'))\n",
    "    ])),\n",
    "    ('Stacking Classifier', StackingClassifier(estimators=[\n",
    "        ('rf', RandomForestClassifier(random_state=seed)),\n",
    "        ('knn', KNeighborsClassifier())\n",
    "    ], final_estimator=LogisticRegression()))\n",
    "]\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=8)\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models:\n",
    "    if name == 'TabNetClassifier':\n",
    "        X_train_np = X_train.values  # Convert DataFrame to NumPy array\n",
    "        cv_scores = cross_val_score(model, X_train_normalized_df, y_train, cv=kf, scoring='accuracy')\n",
    "    else:\n",
    "        cv_scores = cross_val_score(model, X_train_normalized_df, y_train, cv=kf, scoring='accuracy')\n",
    "\n",
    "    mean_accuracy = np.mean(cv_scores)\n",
    "    print(f\"{name} accuracy: {mean_accuracy} \")\n",
    "    std_accuracy = np.std(cv_scores)\n",
    "\n",
    "    results.append((name, mean_accuracy, std_accuracy))\n",
    "\n",
    "sorted_results = sorted(results, key=lambda item: item[1], reverse=True)\n",
    "\n",
    "summary = []\n",
    "for rank, (model_name, mean_accuracy, std_accuracy) in enumerate(sorted_results):\n",
    "    summary.append([model_name, rank + 1, mean_accuracy, std_accuracy])\n",
    "\n",
    "summary_df = pd.DataFrame(summary, columns=['Model', 'Rank', 'Mean Accuracy', 'Std Deviation'])\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized_df.to_csv('X_train_norm_pca.csv', index=False)\n",
    "X_test_normalized_df.to_csv('X_test_norm_pca.csv', index=False)\n",
    "X_train_normalized_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
